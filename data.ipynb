{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Getting job requerments function\n",
    "def get_job_requerments(job : str, outputfile: str, start_page : int, end_page) -> None:\n",
    "    \n",
    "    # initializing lists with information about job\n",
    "    jobs_content = []  # list with page content about job\n",
    "    jobs_names = []  # list with names of the jobs\n",
    "    jobs_links = []  # list with links to the job page\n",
    "    all_jobs_links = [] # list of all jobs links\n",
    "    \n",
    "    # going through all pages at hh.ru site and collecting links that\n",
    "    # refer to job variable and also get content from them\n",
    "    for page_number in range(start_page, end_page):\n",
    "        # creating link for our job and current page\n",
    "        jobs_page = 'https://spb.hh.ru/search/vacancy?area=2&st=searchVacancy&text=' + job.lower() + '&page=' + str(\n",
    "            page_number)\n",
    "    \n",
    "        # getting current hh.ru page with our job\n",
    "        page = requests.get(jobs_page, headers={'User-Agent': 'Custom'})\n",
    "        \n",
    "        # clearing jobs links list\n",
    "        jobs_links.clear()\n",
    "    \n",
    "        # cheking if page is ready to\n",
    "        # bring us some data, else getting error code\n",
    "        if page.status_code == 200:\n",
    "            # parsing page using Beautiful soup\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "            # forming vacancies list from link that\n",
    "            # refer to partucular vacancy\n",
    "            vacancies_list = soup.find_all('a', {'class': 'bloko-link HH-LinkModifier'})\n",
    "    \n",
    "            # cheking if vacancies list is not empty\n",
    "            # and if so, getting vacancy name\n",
    "            # and vacancy link, then retrieve data from vacancy page\n",
    "            if len(vacancies_list) > 0:\n",
    "                # collecting links and names of vacancies\n",
    "                for vacancy in vacancies_list:\n",
    "                    jobs_names.append(vacancy.text)\n",
    "                    if vacancy['href']:\n",
    "                        jobs_links.append(vacancy['href'])\n",
    "                        all_jobs_links.append(vacancy['href'])\n",
    "                    else:\n",
    "                        print('No job link')\n",
    "                        jobs_links.append(None)\n",
    "    \n",
    "                # getting page content for each vacancy\n",
    "                for link in jobs_links:\n",
    "                    # getting vacancy page\n",
    "                    job_page = requests.get(link, headers={'User-Agent': 'Custom'})\n",
    "    \n",
    "                    # checking if page is ready to\n",
    "                    # bring us some data, else getting error code\n",
    "                    if job_page.status_code == 200:\n",
    "                        # parsing vacancy page using Beautiful Soup\n",
    "                        job_soup = BeautifulSoup(job_page.text, 'html.parser')\n",
    "    \n",
    "                        # getting vacancy page content\n",
    "                        page_content = job_soup.find('div', {'class': 'g-user-content'})\n",
    "    \n",
    "                        # forming jobs content list\n",
    "                        jobs_content.append(page_content)\n",
    "                    else:\n",
    "                        print(\"Something wrong with the page: \", job_page.status_code)\n",
    "                        print('vacancy problem')\n",
    "            else:\n",
    "                print('No items in vacancies_list')\n",
    "        else:\n",
    "            print('Something wrong with page: ', page.status_code)\n",
    "            print('GEneral page problem')\n",
    "    \n",
    "    # initializing list with all data about vacancies\n",
    "    data_list = []\n",
    "    \n",
    "    # going through jobs contents and splitting it\n",
    "    # by <strong>. So we get all important headings\n",
    "    # and will be able to get requesments, conditions\n",
    "    # and responsibilities\n",
    "    for job_content in jobs_content:\n",
    "        data_list.append(re.split('<strong>', str(job_content).lower()))\n",
    "    \n",
    "    # initializing lists with information about vacancy\n",
    "    jobs_treb = []  # list of vacancy requerments\n",
    "    jobs_usl = []  # list of vancy conditions\n",
    "    jobs_obyaz = []  # list of vacancy responsibilities\n",
    "    jobs_desc = []  # list of vacancy desctiption\n",
    "    \n",
    "    # going through splitted data and getting\n",
    "    # requerments, conditions, responsibilities\n",
    "    # and descriptions\n",
    "    for job in data_list:\n",
    "        # getting descriptiong because\n",
    "        # it is always first in the split\n",
    "        jobs_desc.append(job[0])\n",
    "    \n",
    "        # initializing flags witch note if there is\n",
    "        # one of requesments, conditions and responsibilities\n",
    "        # in the split\n",
    "        treb_flag = False\n",
    "        obyaz_flag = False\n",
    "        usl_flag = False\n",
    "    \n",
    "        # checking if there is one of three conditions\n",
    "        # in our split and if so, adding this conditions\n",
    "        # to corresponding lists\n",
    "        for job_content in job:\n",
    "            if job_content.startswith('требования'):\n",
    "                jobs_treb.append(job_content)\n",
    "                treb_flag = True\n",
    "    \n",
    "            if job_content.startswith('обязанности'):\n",
    "                jobs_obyaz.append(job_content)\n",
    "                obyaz_flag = True\n",
    "    \n",
    "            if job_content.startswith('условия'):\n",
    "                jobs_usl.append(job_content)\n",
    "                usl_flag = True\n",
    "    \n",
    "        # if we haven't found any conditions\n",
    "        # we add None item to corresponding list\n",
    "        if treb_flag == False:\n",
    "            jobs_treb.append(None)\n",
    "    \n",
    "        if obyaz_flag == False:\n",
    "            jobs_obyaz.append(None)\n",
    "    \n",
    "        if usl_flag == False:\n",
    "            jobs_usl.append(None)\n",
    "    \n",
    "    # initializing lists with clean data about requerments, conditions\n",
    "    # and responsibilities\n",
    "    new_jobs_treb = []  # list of vacancy requerments\n",
    "    new_jobs_obyaz = []  # list of vacancy responsibilities\n",
    "    new_jobs_usl = []  # list of vacancy conditions\n",
    "    \n",
    "    \n",
    "    # Cleaning informations funcition\n",
    "    # Arguments:\n",
    "    #  Data we want to clean and list where we want\n",
    "    # .  to put this data\n",
    "    # . (data, data_list)\n",
    "    #  Returns:\n",
    "    # .  None\n",
    "    def clear_data(data, data_list):\n",
    "        # going through the data and firstly cleaning\n",
    "        # out of three possible conditions, then split\n",
    "        # out data by html tags to form list or conditions\n",
    "        # and cleanign data out of usless symbols\n",
    "        for elem in data:\n",
    "            if elem != None:\n",
    "                # cleaning of three conditions\n",
    "                item = re.sub('требования', '', elem)\n",
    "                item2 = re.sub('к кандидату', '', item)\n",
    "                item3 = re.sub('к кандидатам', '', item2)\n",
    "                item4 = re.sub('условия', '', item3)\n",
    "                item5 = re.sub('обязанности', '', item4)\n",
    "    \n",
    "                # splitting by html tags\n",
    "                splited_items = re.split(r'<.*?>', item5)\n",
    "    \n",
    "                # initializing list with clean items\n",
    "                cleared_items = []\n",
    "    \n",
    "                # going through splitted items, cleaning\n",
    "                # them and adding to cleared items list\n",
    "                for item in splited_items:\n",
    "                    cleared_items.append(re.sub(r'[^\\w\\d\\s]+', '', re.sub(r'\\s+', ' ', re.sub(r'<.*?>', '', item))))\n",
    "    \n",
    "                # deleating all empty items\n",
    "                while (\"\" in cleared_items):\n",
    "                    cleared_items.remove(\"\")\n",
    "    \n",
    "                # deleating all space items\n",
    "                while (\" \" in cleared_items):\n",
    "                    cleared_items.remove(\" \")\n",
    "    \n",
    "                # adding cleaned items to data_list\n",
    "                data_list.append(cleared_items)\n",
    "            else:\n",
    "                # if no element in data, adding None\n",
    "                data_list.append(None)\n",
    "    \n",
    "    \n",
    "    # claning information\n",
    "    clear_data(jobs_obyaz, new_jobs_obyaz)\n",
    "    clear_data(jobs_usl, new_jobs_usl)\n",
    "    clear_data(jobs_treb, new_jobs_treb)\n",
    "    \n",
    "    # initializing list for cleaned\n",
    "    # description data\n",
    "    new_jobs_desc = []\n",
    "    \n",
    "    # going through all descriptions\n",
    "    # and cleaning it\n",
    "    for desc in jobs_desc:\n",
    "        # if something in descriotion, cleaning\n",
    "        # it, overwise, adding None\n",
    "        if desc != None:\n",
    "            item = re.sub(r'[^\\w\\d\\s]+', '', re.sub(r'\\s+', ' ', re.sub(r'<.*?>', '', desc)))\n",
    "            new_jobs_desc.append(item)\n",
    "        else:\n",
    "            new_jobs_desc.append(None)\n",
    "            \n",
    "    all_data = new_jobs_obyaz\n",
    "    \n",
    "    new_all_d = pd.Series(all_data)\n",
    "    \n",
    "    d = new_all_d.dropna()\n",
    "    \n",
    "    with open(outputfile, 'a') as f:\n",
    "        for elem in d:\n",
    "            f.write('@'.join(elem))\n",
    "            \n",
    "def read_info(file : str, tag : str):\n",
    "    with open(file) as f:\n",
    "        data = (f.read()).split('@')\n",
    "        \n",
    "        y_data = [tag for x in range(len(data))]\n",
    "    \n",
    "    return data, y_data\n",
    "\n",
    "# driver_data, y_driver_data = read_info('driver_data.txt', 'driver')\n",
    "meneger_sells_data, y_meneger_sells_data = read_info('meneger_sells', 'meneger_sells')\n",
    "web_development_data, y_web_development_data = read_info('web development', 'web development')\n",
    "data_bases_data, y_data_bases_data = read_info('data bases', 'data bases')\n",
    "old_web_developer_data, y_old_web_developer_data = read_info('web_development_data.txt', 'web development')\n",
    "# network_admin_data, y_network_data = read_info('Network admin', 'Network admin')\n",
    "# system_engeneer_data, y_system_engeneer_data = read_info('System engeneer', 'System engeneer')\n",
    "# information_security, y_information_security = read_info('Information security', 'Information security')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n",
      "No items in vacancies_list\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "get_job_requerments('Базы данных', 'data bases', start_page=31, end_page=50)\n",
    "get_job_requerments('Менеджер по продажам', 'meneger_sells', start_page=31, end_page=50)\n",
    "get_job_requerments('веб разработчик', 'web development', start_page=31, end_page=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "get_job_requerments('Сетевой администратор', 2, 'Network admin')\n",
    "get_job_requerments('Системный инженер', 4, 'System engeneer')\n",
    "get_job_requerments('Специалист по информационной безопасности', 6, 'Information security')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "def preprocess_text(data_list):\n",
    "    new_list = []\n",
    "    for item in data_list:\n",
    "        tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "        new_data = tokenizer.tokenize(item)\n",
    "        \n",
    "        data = [''.join(mystem.lemmatize(token)) for token in new_data]\n",
    "        tokens = [token for token in data if token not in russian_stopwords\\\n",
    "                  and token != ' '\\\n",
    "                  and token.strip() not in punctuation]\n",
    "        \n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        new_text = re.sub('\\n', '', text)\n",
    "        \n",
    "        new_list.append(new_text)\n",
    "    return new_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "2601 2803 2746\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "new_web_develoment_data = preprocess_text(web_development_data) + preprocess_text(old_web_developer_data)\n",
    "\n",
    "\n",
    "new_meneger_sells_data = preprocess_text(meneger_sells_data)\n",
    "\n",
    "new_data_bases_data = preprocess_text(data_bases_data)\n",
    "\n",
    "print(len(new_web_develoment_data) ,len(new_meneger_sells_data), len(new_data_bases_data))\n",
    "#new_network_admin_data = preprocess_text(network_admin_data)\n",
    "\n",
    "#new_system_engeneer_data = preprocess_text(system_engeneer_data)\n",
    "\n",
    "#new_information_security = preprocess_text(information_security)\n",
    "\n",
    "all_data = new_web_develoment_data +  new_meneger_sells_data + new_data_bases_data\n",
    "                                                                                             \n",
    "all_y_data = y_web_development_data + y_old_web_developer_data + y_meneger_sells_data + y_data_bases_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "tfvect = TfidfTransformer()\n",
    "\n",
    "all_data_transformed = vectorizer.fit_transform(all_data)\n",
    "\n",
    "data_transformed = tfvect.fit_transform(all_data_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_transformed, all_y_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 29
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.7217068645640075\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predicted))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:  1.5min finished\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0.7894995093228655"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 96
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer(min_df = 3, stop_words=russian_stopwords)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(random_state=1))])\n",
    "\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_y_data)\n",
    "\n",
    "grid_s = GridSearchCV(pipe, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_s_fit = grid_s.fit(X_train, y_train)\n",
    "\n",
    "predicted = grid_s_fit.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, predicted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    5.1s finished\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0.7161410018552876"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 95
    }
   ],
   "source": [
    "new_pipe = Pipeline([('vect', CountVectorizer(min_df = 3, stop_words=russian_stopwords)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('NB', MultinomialNB())])\n",
    "\n",
    "new_parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'NB__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "new_grid_s = GridSearchCV(new_pipe, new_parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "new_grid_s_fit = new_grid_s.fit(X_train, y_train)\n",
    "\n",
    "new_predicted = new_grid_s_fit.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, new_predicted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=3,\n                                 ngram_range=(1, 2), preprocessor=None,\n                                 stop_words=['и', 'в', 'во', 'не', 'что', 'он',\n                                             'на', 'я', 'с', 'со', 'как', 'а',\n                                             'то', 'все', 'она', 'так', 'его',\n                                             'но', 'да', 'ты', 'к', 'у', 'же',\n                                             'вы', 'за', 'бы', 'по', 'только',\n                                             'ее', 'мне', ...],\n                                 strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=False)),\n                ('NB',\n                 MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))],\n         verbose=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 66
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=3,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=['и', 'в', 'во', 'не', 'что', 'он',\n                                             'на', 'я', 'с', 'со', 'как', 'а',\n                                             'то', 'все', 'она', 'так', 'его',...\n                                 tokenizer=None, vocabulary=None)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 SVC(C=1, break_ties=False, cache_size=200, class_weight=None,\n                     coef0=0.0, decision_function_shape='ovr', degree=3,\n                     gamma='scale', kernel='rbf', max_iter=-1,\n                     probability=False, random_state=1, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 97
    }
   ],
   "source": [
    "grid_s_fit.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "input_requerments = ['умение продавать товары',\n",
    "                     'общительность при общении с клиентами']\n",
    "\n",
    "def make_prediction(requerments: list):\n",
    "    new_input = preprocess_text(requerments)\n",
    "        \n",
    "    text = ' '.join(new_input)\n",
    "\n",
    "    \n",
    "    predict = new_grid_s_fit.predict([text])\n",
    "    \n",
    "    return predict\n",
    "    \n",
    "pred = make_prediction(input_requerments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['meneger_sells'], dtype='<U15')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 108
    }
   ],
   "source": [
    "pred\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}